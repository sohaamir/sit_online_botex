botex: Using LLMs as Experimental Participants in oTree

Overview
Welcome to botex, a new Python package that leverages the power of large language models (LLMs) as participants in oTree experiments.

botex takes a novel approach to integrating LLMs into behavioral experiments. Rather than relying on predefined prompts,1 botex bots dynamically interact with their experimental environment by scraping their respective oTree participant pages. This approach allows them to infer the experimental flow solely from the webpage's textual content. By aligning bot behavior directly with the experimental interface, botex eliminates potential discrepancies between human and bot designs. This not only opens up exciting opportunities to explore LLM behavior but also positions LLMs as a powerful tool for developing and pre-testing experiments intended for human participants.

botex Workflow

Why Choose botex?
botex's innovative approach offers several advantages:

Alignment: By scraping oTree pages, LLMs respond to the same interface as human participants, ensuring consistency in experimental design.
Pre-testing: LLMs can act as intelligent pre-testers, providing valuable feedback during the design phase of human-centric experiments.
Behavioral Insights: Explore how LLMs interact and respond under experimental conditions designed for humans.
Current Capabilities and Limitations
While botex provides robust functionality for standard oTree forms, its reliance on web scraping introduces certain constraints:

Standardized oTree Designs: The oTree frameworkâ€™s rigidity ensures compatibility, but customized HTML forms may require adjustments.
Future Enhancements: We aim to extend support to custom HTML forms. However, some degree of standardization by users will likely be necessary for seamless integration.
See Getting Started for a quick start guide.

Usable LLMs
For interfacing with LLMs, botex offers two options

litellm: Allows the use of various commercial LLMs
llama.cpp: Allows the use of local (open source) LLMs
The model that you use for inference has to support structured outputs. We have tested botex with the following LLMs:

Vendor	Model	Link	Status	Notes
OpenAI	gpt-4o-2024-08-06 and later	OpenAI API	OK	Requires at least paid user tier 1
OpenAI	gpt-4o-mini-2024-07-18 and later	OpenAI API	OK	Requires at least paid user tier 1
Google	gemini/gemini-1.5-flash-8b	Google AI Studio	OK	1,500 requests per day are free
Google	gemini/gemini-1.5-flash	Google AI Studio	OK	1,500 requests per day are free
Google	gemini/gemini-1.5-pro	Google AI Studio	OK	50 requests per day are free (not usable for larger experiments in the free tier)
Open Source	llama-3.1-8b-instruct-Q8_0.gguf	Hugging Face	OK	Run with llama.cpp
Open Source	Mistral-7B-Instruct-v0.3.Q4_K_M.gguf	Hugging Face	OK	Run with llama.cpp
Open Source	qwen2.5-7b-instruct-q4_k_m.gguf	Hugging Face	OK	Run with llama.cpp
If you have success running botex with other models, please let us know so that we can add them to the list.

Paper
If you use botex in your research, please cite its accompanying paper:

Fikir Worku Edossa, Joachim Gassen, and Victor S. Maas (2024): Using Large Language Models to Explore Contextualization Effects in Economics-Based Accounting Experiments. Working Paper. SSRN.

Getting Started

Requirements
If you want to use botex to create LLM participants for your own oTree experiments, you need the following:

A working python environment >= 3.10, pip and preferably a virtual environment.
Google Chrome for scraping the oTree participant pages.

Install botex
Then, activate the virtual environment and install the current stable version of the package in it:
`pip install botex`

Install oTree
The easiest way to get botex up and running is to use its command line interface. It guides you through the process to start botex on a running oTree instance. To set oTree up, you can run the following in the same virtual environment that you set up botex in:
```
pip install otree # You might observe a pip dependency error, but you can ignore it
otree startproject otree # Say yes for sample games
cd otree 
otree devserver
```

Using the botex Command Line Interface
Now, you have a locally running oTree sever. Switch to another terminal with the same virtual environment activated. Then start the botex command line interface by running botex. You should see the following output:

```
(.venv) user@host:~/github/project$ botex
botex database file not provided. Defaulting to 'botex.sqlite3'
oTree server URL not provided. Defaulting to 'http://localhost:8000'
No LLM provided. Enter your model string here ("llamacpp" if you are using
llama.cpp) or press enter to accept the default [gemini/gemini-1.5-flash]: 
```

Assuming that you are fine with using the Google Gemini model, after pressing Enter you need to provide an API key. If you do not have one yet, you can get a free one from the Google AI Studio. After entering the key, you can select an oTree experiment from the examples offered by the fresh oTree installation:
```
Enter the API key for your LLM model (for the Gemini model,
you can get a free API key at https://aistudio.google.com/apikey): ***

Available session configurations:
1: guess_two_thirds
2: survey
Select a configuration by number: 
```

We suggest that you choose the Guess Two Thirds game.
```
Select a configuration by number: 1

Selected session configuration: guess_two_thirds
Enter number of human participants [0]: 
```

While this is up to you, we suggest that you play along, so select one human participant.

```
Selected session configuration: guess_two_thirds
Enter number of human participants [0]: 1
Number of participants: 3
Number of human participants: 1
Session 'd8nqycbk' initialized
Human URLs: ['http://localhost:8000/InitializeParticipant/z0z34nrg']
You can monitor its progress at http://localhost:8000/SessionMonitor/d8nqycbk
Starting bots on session...
```

Running the Experiment
The botex bots will now start to scrape their oTree participant pages and play the game. You can monitor their progress by opening the session monitor URL in your browser. You should see something like this:

Session Monitor
You will see that the bots already made their first move and are waiting for you ('P2'). Time to act. Open the human URL in your browser and play the game. You will see the bots' moves in the session monitor as you progress.

Exporting botex Data
After you and the bots have completed the game, you can download the oTree data by using the oTree web frontend just like you would for a 'normal' experiment. In addition, the app asks whether you want to export the botex data. We suggest that you do, providing a file name to save the LLM response data in the current working directory.
```
Session complete.
You can view the oTree data at http://localhost:8000/SessionData/d8nqycbk
Download the oTree data at: http://localhost:8000/export
Enter CSV file name to export botex data to or press Enter to skip: two_thirds.csv
```

With this, the botex app terminates. You have run your first oTree experiment with LLM participants. Congratulations!

Take a look at the resulting CSV file. It contains the responses from the two bot instances and the reasoning for their answers. The data should look something like this:

session_id	participant_id	round	question_id	answer	reason
d8nqycbk	tojkfg7i	1	id_guess	66	"I will start by guessing 2/3 of 50, which is approximately 33. However, I expect others to do the same, so I'll choose a slightly higher number hoping to be closer to 2/3 of the average."
d8nqycbk	tojkfg7i	2	id_guess	30	"I reasoned that many players will start with a guess of 0, assuming everyone is rational. But then many will increase their number, so I choose a number slightly above 0 to try and outsmart my competitors. The previous round's 2/3 average was 36. I'll try a much lower number this time."
d8nqycbk	tojkfg7i	3	id_guess	10	"Given the previous rounds' results and the tendency for players to adjust their guesses, I expect most guesses to cluster around 0. Therefore, I'm choosing a low number to increase my chances of winning."
d8nqycbk	czkz2bbj	1	id_guess	66	"I am choosing a number slightly below 67, since the other participants would probably consider this iterative reasoning and would bid even lower."
d8nqycbk	czkz2bbj	2	id_guess	20	"In the first round, the average was close to 60. If everyone assumes everyone else is rational, then the average will continue to decrease towards 0. However, since the number must be between 0 and 100, there is a lower bound to the guess. This time I will guess 20."
d8nqycbk	czkz2bbj	3	id_guess	10	"Given the previous rounds, it's likely that participants will continue to reduce their guesses. I am choosing a very low number to try and win, acknowledging that if everyone does this, the average will be very low."

You see that, in this run, our LLM bots were not particularly smart in the first round but then quickly adapted their strategies in the later rounds.

Next Steps
If you are interested in learning how to use the Python API of botex in your own code, we suggest that you continue with the tutorial Run an oTree experiment with botex. If you are interested in a more advanced use case and how different LLMs perform in the Guess Two Thirds game, you can thereafter continue with the tutorial Using botex to benchmark LLM performance in oTree experiments.

Run an oTree Experiment with botex

Starting oTree via Python
When you want to run an oTree experiment with botex bots, you need to have access to a running oTree instance that is able to host the experiment that you want to run.

In the getting started section, we have seen how to start an oTree server via the command line. Now, we will use botex to start an oTree server instance via Python. This is useful if you want to automate the process of running an experiment and collecting its data.

The following code snippet shows how to start an oTree server via Python:
```
import botex

# botex is silent by default. If you want to understand
# what it is doing, it is useful to enable logging 
import logging
logging.basicConfig(level=logging.INFO)

# Start the oTree server
botex.start_otree_server(project_path="otree")
```

Adjust the project_path parameter to where you installed your oTree project. The above starts a development oTree server without authentication. If you want to require authentication (recommended for publicly accessible servers), you can set the auth_level parameter of start_otree_server to 'STUDY'. In this case, you should also set the parameters rest_key and admin_password.

Running the snippet from above, you should see the following output:
```
INFO:botex:oTree server started successfully with endpoint 'http://localhost:8000'
```

Please note that the oTree sever instance will terminate when your script terminates.

Retrieve the Available Session Configurations
Before you can run an experiment, you might want to know which session configurations are available. The extension of the code shows how to retrieve the available session configurations from a running oTree server:
```
import botex

import logging
logging.basicConfig(level=logging.INFO)

# Start the oTree server
otree_process = botex.start_otree_server(project_path="otree")

# Retrieve the session configurations
session_configs = botex.get_session_configs(
    otree_server_url="http://localhost:8000"
)
print(session_configs)

# Stop the oTree server
botex.stop_otree_server(otree_process)
```

If you are using a locally running oTree server, you can omit the otree_server_url parameter. When you are accessing a remotely running oTree server, you need to provide its URL (including the correct port if required). If you try to access an oTree server with authentication, you also need to provide the rest_key parameter. Besides querying the session config from the server, we also added a call to stop_otree_server to gracefully terminate the oTree server instance before ending the script.

The output of the above script should look like this:
```
INFO:botex:oTree server started successfully with endpoint 'http://localhost:8000'
[{'real_world_currency_per_point': 1.0, 'participation_fee': 0.0, 'doc': '', 
'name': 'guess_two_thirds', 'display_name': 'Guess 2/3 of the Average', 
'app_sequence': ['guess_two_thirds', 'payment_info'], 
'num_demo_participants': 3}, 
{'real_world_currency_per_point': 1.0, 'participation_fee': 0.0, 'doc': '', 
'name': 'survey', 'app_sequence': ['survey', 'payment_info'], 
'num_demo_participants': 1, 'display_name': 'survey'}]
INFO:botex:oTree server stopped.
```

Initialize a Session
Once you know which session configurations are available, you can initialize a session. The extension of the code shows how to do this:
```
import botex

import logging
logging.basicConfig(level=logging.INFO)

# Start the oTree server
otree_process = botex.start_otree_server(project_path="otree")

# Retrieve the session configurations
session_configs = botex.get_session_configs(
    otree_server_url="http://localhost:8000"
)

# Initialize a session
session = botex.init_otree_session(
    config_name=session_configs[0]['name'], # "guess_two_thirds"
    npart = 3,
    otree_server_url="http://localhost:8000",
    botex_db = 'botex.sqlite3'
)

print(session)

# Stop the oTree server
botex.stop_otree_server(otree_process)
```

Again, adjust the otree_server_url parameter and add rest_key as required. init_otree_session() requires the session config name and the number of participants to include. When changing this parameter, make sure that the experiment supports it. E.g., the Guess Two Thirds game provided as an example with the oTree installation requires participants to be multiples of three. In addition, we need to provide a file for the SQLite database where botex stores its data. If the file does not exist, it will be created. If it exists, the data will be appended.

init_otree_session() returns a dict with session data, including the session ID and the participant URLs. The output of the above script should look like this:
```
INFO:botex:oTree server started successfully with endpoint 'http://localhost:8000'
{'session_id': 'lry96cc8', 
'participant_code': ['dlg5vdbq', 'j8y24ubc', 'pbewmoh2'], 
'is_human': [False, False, False], 
'bot_urls': ['http://localhost:8000/InitializeParticipant/dlg5vdbq', 
'http://localhost:8000/InitializeParticipant/j8y24ubc', 
'http://localhost:8000/InitializeParticipant/pbewmoh2'], 
'human_urls': []}
INFO:botex:oTree server stopped.
```

You see that we initialized a session with three participants, all of which are bots. The bot URLs are provided in the bot_urls list.

Running botex Bots on a Session
Once you have initialized a session, you can run the botex bots on it. Let's extend our code:
```
import botex

import logging
logging.basicConfig(level=logging.INFO)

# Start the oTree server
otree_process = botex.start_otree_server(project_path="otree")

# Retrieve the session configurations
session_configs = botex.get_session_configs(
    otree_server_url="http://localhost:8000"
)

# Initialize a session
session = botex.init_otree_session(
    config_name=session_configs[0]['name'], # "guess_two_thirds"
    npart = 3,
    otree_server_url="http://localhost:8000",
    botex_db = 'botex.sqlite3'
)

# Run the bots on the session
botex.run_bots_on_session(
    session_id=session['session_id'],
    botex_db = 'botex.sqlite3',
    model="gemini/gemini-1.5-flash",
    api_key="***"
)

# Stop the oTree server
botex.stop_otree_server(otree_process)
```

If it works, you will be greeted with a very long log ouput, detailing the botex bots' interactions with the oTree server. If you want to see less of this, you can adjust the logging level to logging.WARNING or disable logging. However, we suggest that you take a good look at the log output to familiarize yourself with the workflow of the botex bots.

The output should start and end like this:
```
INFO:botex:oTree server started successfully with endpoint 'http://localhost:8000'
INFO:botex:Running bots on session q4ntmcdt. You can monitor the session at http://localhost:8000/SessionMonitor/q4ntmcdt
INFO:botex:Running bot with parameters: {"botex_db": "botex.sqlite", "session_id": "q4ntmcdt", "full_conv_history": false, "model": "gemini/gemini-1.5-flash", "api_key": "******", "api_base": null, "user_prompts": null, "throttle": false, "otree_server_url": "http://localhost:8000", "url": "http://localhost:8000/InitializeParticipant/372wu8py"}
INFO:botex:Running bot with parameters: {"botex_db": "botex.sqlite", "session_id": "q4ntmcdt", "full_conv_history": false, "model": "gemini/gemini-1.5-flash", "api_key": "******", "api_base": null, "user_prompts": null, "throttle": false, "otree_server_url": "http://localhost:8000", "url": "http://localhost:8000/InitializeParticipant/ay4nos1w"}
INFO:botex:Running bot with parameters: {"botex_db": "botex.sqlite", "session_id": "q4ntmcdt", "full_conv_history": false, "model": "gemini/gemini-1.5-flash", "api_key": "******", "api_base": null, "user_prompts": null, "throttle": false, "otree_server_url": "http://localhost:8000", "url": "http://localhost:8000/InitializeParticipant/nr4th0it"}
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=****** "HTTP/1.1 200 OK"
INFO:botex:Bot's response to start message:
{
    "task": "I am participating in an online survey or experiment.  My task is to provide JSON formatted responses based on prompts that will include a summary of the survey/experiment so far and any new information to be added to the summary. This summary will function as my memory throughout the experiment.  Each prompt also includes scraped text from a webpage related to the survey/experiment.  I will analyze the text, answer any questions or complete tasks within it, and incorporate any relevant information into the updated summary.  If compensation is mentioned for participants, I will consider that compensation to apply to me as well. I will provide responses only in JSON format, adhering to the specified schema.",
    "understood": true
}

[... maaaaany more lines ...]

INFO:botex:Bot's final remarks about experiment:
{
    "confused": false,
    "remarks": "The experiment was well-structured and engaging. My strategy of iteratively reducing my guess in round 3 was based on observing that lower numbers were generally more successful in the previous rounds. However, it didn't prove to be as effective as hoped. The instructions were clear and the payoff system was straightforward. The information about the experiment administrator's instructions and the oTree implementation details were interesting to learn, although as a participant, they were not directly relevant to my participation in the game. Overall, this was a successful experiment design and the implementation in this conversation worked effectively.  The JSON format for responses worked well and was easy to use."
}
INFO:botex:Bot finished.
INFO:botex:Data stored in botex database.
INFO:botex:oTree server stopped.
```

When inspecting the log output, you will likely notice a warning like this:
```
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=****** "HTTP/1.1 429 Too Many Requests"
WARNING:botex:Litellm completion failed, error: 'litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {
  "error": {
    "code": 429,
    "message": "Resource has been exhausted (e.g. check quota).",
    "status": "RESOURCE_EXHAUSTED"
  }
}
'
INFO:botex:Retrying with throttling.
```

This is because the free tier of the Google Gemini model has per minute rate limits for requests. On encountering this error, botex automatically applies an exponential backoff strategy, meaning that it keeps retrying with increasing delays until the request is successful. If you want to avoid the warning, you can set the throttle parameter of run_bots_on_session to True. This will cause botex to throttle the requests to the model by default.

Exporting the Data
After running the experiment and before shutting down the oTree server, you might want to export the data, both from oTree and from botex. So here is our final extension of the code:
```
import botex

# botex is relatively silent by default. If you want to understand
# what it is doing, it is useful to enable logging. 
# We set it to WARNING here so that we will be only informed if 
# something goes wrong.
import logging
logging.basicConfig(level=logging.WARNING)

# Will be created in the current directory if it does not exist
BOTEX_DB = "botex.sqlite3"

# Path to your oTree project folder if you want the code to start the server
OTREE_PROJECT_PATH = "otree"

# Change the oTree URL if you are using a remote server
OTREE_URL = "http://localhost:8000"

# If you use a higher oTree authentication level, 
# you need to set the following
OTREE_REST_KEY = None
OTREE_ADMIN_NAME = None
OTREE_ADMIN_PASSWORD = None

# LLM model vars
LLM_MODEL = "gemini/gemini-1.5-flash"
LLM_API_KEY = "******"

# Start the oTree server - if not using an already running server
otree_process = botex.start_otree_server(project_path=OTREE_PROJECT_PATH)

# Get the available session configurations from the oTree server
session_configs = botex.get_session_configs(otree_server_url=OTREE_URL)

# Initialize a session
session = botex.init_otree_session(
    config_name=session_configs[0]['name'], # "guess_two_thirds"
    npart = 3,
    otree_server_url=OTREE_URL,
    otree_rest_key=OTREE_REST_KEY,
    botex_db = BOTEX_DB
)

# Run the bots on the session
print(
    f"Starting bots. You can monitor their progress at "
    f"http://localhost:8000/SessionMonitor/{session['session_id']}"
)
botex.run_bots_on_session(
    session_id=session['session_id'],
    botex_db=BOTEX_DB,
    model=LLM_MODEL,
    api_key=LLM_API_KEY,
    throttle=True
)

# Export oTree data - you only need to set admin name and password if you
# have set a higher authentication level ('DEMO' or 'STUDY') in oTree
botex.export_otree_data(
    "two_thirds_otree_wide.csv",
    admin_name = OTREE_ADMIN_NAME,
    admin_password = OTREE_ADMIN_PASSWORD
)
botex.normalize_otree_data(
    "two_thirds_otree_wide.csv", 
    store_as_csv=True,
    exp_prefix="two_thirds_otree"
)

# Export botex data
botex.export_participant_data(
    "two_thirds_botex_participants.csv",
    botex_db=BOTEX_DB
)
botex.export_response_data(
    "two_thirds_botex_responses.csv",
    botex_db=BOTEX_DB,
    session_id=session['session_id']
)

# Stop the oTree server
botex.stop_otree_server(otree_process)
```

To silence botex, we now set the logging level to WARNING. Also, we set throttle to True to avoid that botex nags us about the rate limit rejects. We also refactor the code to use variables for the paths and URLs. This makes it easier to adjust the code to your setup.

The code now starts an oTree server, initializes a session, runs the bots on the session, and exports the data. The data are exported in CSV format. The botex data are exported in two files: one containing the participant data and one containing the responses. The oTree data are exported in wide format and then normalized. The normalized data are stored in a set of CSV files with the prefix two_thirds_otree. You should see all files in your project directory after the code has been run.

This concludes this tutorial. If you want to learn how to run single botex bots using different LLM models to benchmark their performance with oTree experiments and how to evaluate the results, please refer to the next tutorial.
# botex.env - Configuration for botex experiment with llama.cpp

# oTree configuration
OTREE_SESSION_CONFIG=social_influence_task
OTREE_NPARTICIPANTS=5
OTREE_NHUMANS=0

# Skip health check (needed for llama.cpp)
BOTEX_SKIP_HEALTH_CHECK=1

# Path to model file (for reference only - actual path is passed to the server binary)
LLAMACPP_LOCAL_LLM_PATH=models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

# For cloud models (optional)
# API_KEY=your_api_key_here

# llamacpp configuration
LLM_MODEL="llamacpp"
API_BASE="http://localhost:8080"